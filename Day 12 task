from pyspark.sql import functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

silver = spark.read.format("delta").load("/Volumes/workspace/ecommerce/ecommerce_data/silver_events")

# Minimal fix: create price_double column
silver = silver.withColumn("price_double", F.col("price").cast("double"))

# Prepare features
features = silver.withColumn("hour", F.hour("event_time")) \
    .withColumn("day_of_week", F.dayofweek("event_date")) \
    .withColumn("price_log", F.log(F.col("price_double")+1))

assembler = VectorAssembler(inputCols=["hour", "day_of_week"], outputCol="features")
data = assembler.transform(features).select("features", "price_log").dropna()

# Train regression model
lr = LinearRegression(featuresCol="features", labelCol="price_log")
model = lr.fit(data)



import mlflow
import mlflow.spark

mlflow.set_experiment("/Users/barakathullah.r@gmail.com/Day12-Experiment")
mlflow.end_run()
with mlflow.start_run():
    mlflow.log_param("features", ["hour", "day_of_week"])
    mlflow.log_metric("rmse", model.summary.rootMeanSquaredError)
    mlflow.spark.log_model(model, "model", dfs_tmpdir="/Volumes/workspace/ecommerce/ecommerce_data/mlflow_tmp")
