from pyspark.sql import functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
import mlflow
import mlflow.spark

# Load and prepare features
silver = spark.read.format("delta").load("/Volumes/workspace/ecommerce/ecommerce_data/silver_events")
silver = silver.withColumn("price_double", F.col("price").cast("double"))
features = silver.withColumn("hour", F.hour("event_time")) \
    .withColumn("day_of_week", F.dayofweek("event_date")) \
    .withColumn("price_log", F.log(F.col("price_double")+1))
assembler = VectorAssembler(inputCols=["hour", "day_of_week"], outputCol="features")
data = assembler.transform(features).select("features", "price_log").dropna()

# Model definitions
models = [
    ("LinearRegression", LinearRegression(featuresCol="features", labelCol="price_log")),
    ("DecisionTree", DecisionTreeRegressor(featuresCol="features", labelCol="price_log")),
    ("RandomForest", RandomForestRegressor(featuresCol="features", labelCol="price_log", numTrees=10))
]

mlflow.set_experiment("/Users/barakathullah.r@gmail.com/Day13-Experiment")
results = []
evaluator = RegressionEvaluator(labelCol="price_log", predictionCol="prediction", metricName="rmse")

for name, model in models:
    with mlflow.start_run(run_name=name):
        fitted = model.fit(data)
        if hasattr(fitted, "summary"):
            rmse = fitted.summary.rootMeanSquaredError
        else:
            predictions = fitted.transform(data)
            rmse = evaluator.evaluate(predictions)
        mlflow.log_param("model_type", name)
        mlflow.log_metric("rmse", rmse)
        mlflow.spark.log_model(fitted, "model", dfs_tmpdir="/Volumes/workspace/ecommerce/ecommerce_data/mlflow_tmp")
        results.append((name, rmse))

print("Model RMSEs:", results)






from pyspark.ml import Pipeline
from pyspark.ml.regression import RandomForestRegressor

# Use the same assembler and features as before
rf = RandomForestRegressor(featuresCol="features", labelCol="price_log", numTrees=10)
pipeline = Pipeline(stages=[assembler, rf])
pipeline_model = pipeline.fit(features.dropna())

# Optionally, evaluate pipeline model
predictions = pipeline_model.transform(features.dropna())
from pyspark.ml.evaluation import RegressionEvaluator
evaluator = RegressionEvaluator(labelCol="price_log", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("Pipeline RMSE:", rmse)
