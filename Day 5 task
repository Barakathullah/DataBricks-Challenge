from delta.tables import DeltaTable
from pyspark.sql import functions as F

# Load the existing Delta table
#deltaTable = DeltaTable.forName(spark, "nov_events_table")
deltaTable = DeltaTable.forPath(spark, "/Volumes/workspace/ecommerce/ecommerce_data/orders")
# Read incremental data
new_data = (
    spark.read.csv(
        "/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv",
        header=True,
        inferSchema=True
    )
    .limit(100)
)

# Derive product_name from category_code
updates = new_data.withColumn(
    "product_name",
    F.coalesce(
        F.element_at(F.split(F.col("category_code"), "\\."), -1),
        F.lit("Others")
    )
)

# Perform incremental MERGE (upsert)
(
    deltaTable.alias("t")
    .merge(
        updates.alias("s"),
        "t.user_session = s.user_session AND t.event_time = s.event_time"
    )
    .whenMatchedUpdateAll()
    .whenNotMatchedInsertAll()
    .execute()
)

print("Incremental merge completed")


  # Time Travel

# View Delta table history
display(deltaTable.history())

# Read data from a specific version
v0 = (
    spark.read
    .format("delta")
    .option("versionAsOf", 0)
    .table("/Volumes/workspace/ecommerce/ecommerce_data/orders")
)

# Read data from a specific timestamp
yesterday = (
    spark.read
    .format("delta")
    .option("timestampAsOf", "2024-01-01")
    .table("/Volumes/workspace/ecommerce/ecommerce_data/orders")




  # Read version 2 of the table
df_v2 = spark.read.format("delta").option("versionAsOf", 2).load(delta_path)
display(df_v2)

# Or by timestamp
df_old = spark.read.format("delta").option("timestampAsOf", "2026-01-12T10:00:00Z").load(delta_path)



  spark.sql("""
OPTIMIZE delta.`/Volumes/workspace/ecommerce/ecommerce_data/orders` ZORDER BY (user_id, product_id)
""")


            spark.sql("""
VACUUM delta.`/Volumes/workspace/ecommerce/ecommerce_data/orders` RETAIN 168 HOURS
""")




  
