from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Load silver events table
silver = spark.read.format("delta").load("/Volumes/workspace/ecommerce/ecommerce_data/silver_events")

# 1. Descriptive statistics for price
silver.describe(["price"]).show()

# 2. Hypothesis: weekday vs weekend conversion
silver = silver.withColumn("event_date", F.to_date("event_time"))
silver = silver.withColumn("is_weekend", F.dayofweek("event_date").isin([1,7]))
conversion = silver.groupBy("is_weekend", "event_type").count()
display(conversion)

# 3. Correlation between price and purchases
# First, create a conversion_rate column per product
silver = silver.withColumn("price_double", F.col("price").cast("double"))
product_stats = silver.filter(F.col("price_double").isNotNull()).groupBy("product_id").agg(
    F.countDistinct(F.when(F.col("event_type") == "view", "user_id")).alias("views"),
    F.countDistinct(F.when(F.col("event_type") == "purchase", "user_id")).alias("purchases"),
    F.sum(F.when(F.col("event_type") == "purchase", F.col("price_double"))).alias("revenue")
)
product_stats = product_stats.withColumn(
    "conversion_rate",
    F.when(F.col("views") != 0, F.col("purchases") / F.col("views")).otherwise(None)
)

# Correlation between revenue and conversion_rate
corr = product_stats.stat.corr("revenue", "conversion_rate")
print("Correlation between revenue and conversion rate:", corr)

# 4. Feature engineering for ML
features = silver.withColumn("hour", F.hour("event_time")) \
    .withColumn("day_of_week", F.dayofweek("event_date")) \
    .withColumn("price_log", F.log(F.col("price_double")+1)) \
    .withColumn("time_since_first_view",
        F.unix_timestamp("event_time") -
        F.unix_timestamp(F.first("event_time").over(Window.partitionBy("user_id").orderBy("event_time")))
    )
features.select("user_id", "event_time", "hour", "day_of_week", "price_log", "time_since_first_view").show(10)
